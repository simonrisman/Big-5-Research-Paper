# -*- coding: utf-8 -*-
"""COMP550_FinalCode.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1A7-qLW-h6Llri9i8GgAsHqh9QNaJHnhg

## Setup
"""

!pip install convokit
!pip install sklearn
# spacy setup
!python -m spacy download en_core_web_sm
# nltk setup
import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
import convokit
from convokit import Corpus, download

corpora = {}
corpora['CaSiNo'] = Corpus(filename=download('casino-corpus'))
corpora['pfg'] = Corpus(filename=download('persuasionforgood-corpus'))

for name, corpus in corpora.items():
  print("Corpus: ", name)
  corpus.print_summary_stats()
  print()

"""#### Match the labels and normalize Big-Fave scores to a 1-5 scale."""

import numpy as np

new_meta_labels = ['extrovert','agreeable', 'conscientious', 'neurotic', 'open']
old_meta_labels = ['extraversion','agreeableness','conscientiousness', 'emotional-stability', 'openness-to-experiences']

for speaker in corpora['CaSiNo'].iter_speakers():
  meta = speaker.meta['personality']['big-five']
  new_meta = {}
  speaker.meta['SKIP'] = False
  for new,old in zip(new_meta_labels, old_meta_labels):
    new_meta[new] = round(meta[old] * 5/7,1)
  speaker.meta['big-five'] = new_meta

for speaker in corpora['pfg'].iter_speakers():
  new_meta = {}
  speaker.meta['SKIP'] = False
  for label in new_meta_labels:

    if np.isnan(speaker.meta[label]):
      speaker.meta['SKIP'] = True

    new_meta[label] = speaker.meta[label]
  speaker.meta['big-five'] = new_meta


casino_meta = [speaker.meta['big-five'] for speaker in corpora['CaSiNo'].iter_speakers()]
pfg_meta = [speaker.meta['big-five'] for speaker in corpora['pfg'].iter_speakers()]

print(casino_meta[:5])
print(pfg_meta[:5])

"""### Preprocessing

X = All of the speakers utterances

y = Array of feature matrixs for each big-five trait
"""

from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.stem import WordNetLemmatizer


bigfive = ['extrovert','agreeable', 'conscientious', 'neurotic', 'open']
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))

"""
GETTIN THE X
"""

X = []

for corpus in corpora.values():
    for speaker in corpus.iter_speakers():
        speaker_text = ""
        if speaker.meta['SKIP']:
          continue

        for utt in speaker.iter_utterances():
          line = word_tokenize(utt.text)
          line = [word for word in line if word not in stop_words]
          # X.append(" ".join(line))
          speaker_text += " " + " ".join(line)
          utt.text = " ".join(line)
        X.append(speaker_text)

"""
GETTIN THE Y
"""

y = []
for label in bigfive:
  bigfive_labels = []
  for corpus in corpora.values():
      for speaker in corpus.iter_speakers():
        if speaker.meta['SKIP']:
          continue
        bigfive_labels += [speaker.meta['big-five'][label]] # *len(speaker.utterances)
  y.append(bigfive_labels)

X[25]

for i,z in enumerate(y):
  print(bigfive[i].upper())
  print(z[25], " out of 5.0")
  print()

"""We turn this into a binary classification task, splitting data at the mean"""

for i,z in enumerate(y):
  mean = np.average(z)
  z = ['above' if x > mean else 'below' for x in z]
  y[i] = z

from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.pipeline import make_pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import classification_report

bigfive = ['extrovert','agreeable', 'conscientious', 'neurotic', 'open']
clf = SVC()
for i,z in enumerate(y):
  for name in bigfive:
      X_train, X_test, y_train, y_test = train_test_split(X, z, test_size=0.2, random_state=291)

      model = make_pipeline(TfidfVectorizer(lowercase=False, ngram_range=(1,2), max_df=.75),clf)

      model.fit(X_train, y_train)
      y_pred = model.predict(X_test)
      print("--------------------------------------------------------")
      print("\t" + bigfive[i])
      print()
      print(classification_report(y_test, y_pred))
      print("--------------------------------------------------------")

from sklearn.metrics import classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

X = []
y = []
for speak in corpora['CaSiNo'].iter_speakers():
  y += [speak.meta['personality']['svo']]

  for utt in speaker.iter_utterances():
    line = word_tokenize(utt.text)
    line = [word for word in line if word not in stop_words]
    # X.append(" ".join(line))
    speaker_text += " " + " ".join(line)
    utt.text = " ".join(line)
  X.append(speaker_text)

vectorizer = TfidfVectorizer(stop_words=None, ngram_range=(1,3))
X_transformed = vectorizer.fit_transform(X)

logistic = LogisticRegression()
X_train, X_test, y_train, y_test = train_test_split(X_transformed, y, test_size=0.25, random_state=267)



logistic.fit(X_train, y_train)
y_pred = logistic.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(accuracy)
print(y_pred)
print(classification_report(y_pred, y_test))

from google.colab import drive
drive.mount('/content/drive/')
!ls "/content/drive/MyDrive/Colab Notebooks/"

!apt-get -q install texlive-xetex texlive-fonts-recommended texlive-plain-generic

# Replace STUDENT-ID with your student
!jupyter nbconvert --to pdf "/content/drive/MyDrive/Colab Notebooks/COMP550_FinalCode.ipynb"

# Replace STUDENT-ID with your student id below:
from google.colab import files
files.download(f"/content/drive/MyDrive/Colab Notebooks/COMP550_FinalCode.pdf")